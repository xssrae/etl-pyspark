# Desafio - Vaga Analista Junior - ETL Pipeline com PySpark

Este projeto implementa um pipeline ETL (Extract, Transform, Load) utilizando PySpark para integrar dados de clientes e vendas, gerar resumos por cliente e relatÃ³rios financeiros por produto. O sistema ingere dados de fontes heterogÃªneas, realiza limpeza, aplica regras de negÃ³cio complexas e entrega dados estruturados prontos para anÃ¡lise (Data Lake).

## ğŸš€ Funcionalidades

* **IngestÃ£o de Dados:**
    * Leitura de CSV com inferÃªncia de schema (`clientes.csv`).
    * Leitura e parsing manual de arquivos de texto posicional/Fixed-Width (`vendas.txt`).
* **TransformaÃ§Ã£o & Data Quality:**
    * Tratamento de tipos de dados (Inteiros, Decimais com ajuste de escala, Datas).
    * Enriquecimento de dados (CÃ¡lculo de idade e categorizaÃ§Ã£o de faixa etÃ¡ria).
    * Cruzamento de dados (Joins) entre transaÃ§Ãµes e dimensÃµes.
* **Particionamento (Data Lake):**
    * Output detalhado organizado em diretÃ³rios particionados por data (`data_venda=YYYY-MM-DD`), otimizando consultas futuras.
* **Analytics:**
    * GeraÃ§Ã£o de KPIs financeiros por produto e cliente.
    * Insights sobre ticket mÃ©dio e comportamento demogrÃ¡fico.

## ğŸ› ï¸ Tecnologias Utilizadas

* **Linguagem:** Python 3
* **Motor de Processamento:** PySpark (Apache Spark)
* **Bibliotecas Auxiliares:** `csv`, `os`, `shutil`, `unittest` (Testes), `random` (Mock Data).
* **Ambiente:** ExecutÃ¡vel localmente (Windows/Linux/Mac) sem dependÃªncia de instalaÃ§Ã£o completa do Hadoop (Winutils bypass).

---

## ğŸ“‚ Estrutura do Projeto

```text
â”œâ”€â”€ dados/                  # DiretÃ³rio de entrada (Gerado automaticamente)
â”‚   â”œâ”€â”€ clientes.csv        # Cadastro de clientes
â”‚   â””â”€â”€ vendas.txt          # Arquivo posicional legado
â”œâ”€â”€ output/                 # DiretÃ³rio de saÃ­da
â”‚   â”œâ”€â”€ resumo_clientes.csv # KPI consolidado por cliente
â”‚   â”œâ”€â”€ balanco_produtos.csv# KPI consolidado por produto
â”‚   â””â”€â”€ vendas_detalhadas/  # Dataset particionado (Data Lake)
â”œâ”€â”€ etl_pipeline.py         # CÃ³digo principal do Pipeline
â”œâ”€â”€ gerar_dados.py          # Script para gerar clientes fake
â”œâ”€â”€ gerar_vendas_massivo.py # Script para gerar volume de vendas fake
â”œâ”€â”€ test_etl.py             # Testes Automatizados (UnitÃ¡rios)
â””â”€â”€ README.md               # DocumentaÃ§Ã£o
````

---

## â–¶ï¸ Como Executar o Pipeline

Siga os passos abaixo para rodar o projeto no seu ambiente local.

### 1\. PrÃ©-requisitos

Certifique-se de ter o Python instalado. Instale as dependÃªncias necessÃ¡rias:

```bash
pip install pyspark pandas matplotlib
```

### 2\. Gerar Massa de Dados

Como os arquivos de dados brutos nÃ£o sÃ£o versionados, vocÃª deve executar os scripts geradores para criar a pasta `dados/` com informaÃ§Ãµes simuladas:

```bash
# 1. Gera o cadastro de clientes
python gerar_dados.py

# 2. Gera 50.000 registros de vendas (com simulaÃ§Ã£o de churn/inatividade)
python gerar_vendas_massivo.py
```

### 3\. Executar o ETL

Execute o script principal. O Spark processarÃ¡ os arquivos, aplicarÃ¡ as regras de negÃ³cio e salvarÃ¡ os resultados na pasta `output/`.

```bash
python etl_pipeline.py
```

*Ao final, verifique a pasta `output/` para ver os relatÃ³rios CSV e a pasta particionada `vendas_detalhadas/`.*

-----

## âœ… Testes Automatizados

O projeto inclui testes unitÃ¡rios para garantir a integridade da lÃ³gica de transformaÃ§Ã£o e leitura de arquivos posicionais.

Para rodar a suÃ­te de testes:

```bash
python test_etl.py
```

**O que Ã© testado:**

  * Parsing correto das posiÃ§Ãµes do arquivo `vendas.txt` (garantindo que ID, Valor e Data nÃ£o venham corrompidos).
  * LÃ³gica de Join e AgregaÃ§Ã£o (Soma de valores) com dados controlados (Mock).

-----

## ğŸ“„ Exemplos de Arquivos (Input & Output)

### 1\. Entrada: `vendas.txt` (Formato Posicional)

Arquivo sem separadores (vÃ­rgulas ou pipes). O layout Ã© fixo: ID(5), Cliente(5), Produto(5), Valor(8), Data(8).

```text
000010045200100000455020230512  <-- LÃª-se: Venda 1, Cliente 452, Prod 100, R$ 45.50
000020000500102001500020230512  <-- LÃª-se: Venda 2, Cliente 5, Prod 102, R$ 150.00
```

### 2\. SaÃ­da: `resumo_clientes.csv`

```csv
cliente_id,nome,total_vendas,quantidade_vendas,ticket_medio
5,Derrek,25506.01,101,252.53
9,Derby,24507.70,100,245.08
```

### 3\. SaÃ­da: Particionamento de DiretÃ³rios (Data Lake)

O pipeline organiza os dados detalhados simulando a estrutura de um Data Lake (Hive Partitioning), facilitando a leitura por dia especÃ­fico:

```text
output/vendas_detalhadas/
    â”œâ”€â”€ data_venda=2023-01-01/
    â”‚      â””â”€â”€ dados.csv
    â”œâ”€â”€ data_venda=2023-01-02/
    â”‚      â””â”€â”€ dados.csv
    â””â”€â”€ ...
```

-----

## ğŸ›¡ï¸ ResiliÃªncia e Tratamento de Erros

O cÃ³digo foi desenvolvido focando em robustez para ambientes Windows e Linux:

1.  **ValidaÃ§Ã£o de Caminhos:** O script verifica e recria automaticamente as pastas de saÃ­da para garantir idempotÃªncia (pode rodar vÃ¡rias vezes sem erro).
2.  **Try/Except Blocks:** Todas as funÃ§Ãµes crÃ­ticas possuem tratamento de exceÃ§Ã£o para falhar de forma graciosa e informativa.
3.  **Compatibilidade Windows:** Foi implementada uma estratÃ©gia hÃ­brida na carga de dados (coleta via Spark -\> escrita via Python CSV nativo) para contornar a necessidade de binÃ¡rios do Hadoop (`winutils.exe`) no Windows.
