# Desafio de Engenharia de Dados - ETL Pipeline com PySpark

Este projeto implementa um pipeline ETL (Extract, Transform, Load) completo para processamento de dados de vendas e clientes. O sistema ingere dados de fontes heterogÃªneas (CSV e TXT Posicional), realiza limpeza, enriquecimento e entrega insights de negÃ³cios e dados particionados para Data Lake.

## ğŸš€ Funcionalidades

* **IngestÃ£o de Dados:**
    * Leitura de `clientes.csv` (Schema Inferred).
    * Parsing manual de `vendas.txt` (formato Fixed-Width/Posicional).
* **TransformaÃ§Ã£o:**
    * Tratamento de tipos de dados (Inteiros, Decimais, Datas).
    * Enriquecimento: CÃ¡lculo de Idade e categorizaÃ§Ã£o por Faixa EtÃ¡ria.
    * Joins entre dados transacionais e dimensionais.
* **Particionamento (Diferencial):**
    * Output organizado em pastas por data (`data_venda=YYYY-MM-DD`), simulando estrutura de Data Lake.
* **Analytics:**
    * BalanÃ§o financeiro por produto.
    * AnÃ¡lise de comportamento de compra por faixa etÃ¡ria.
    * Ranking de melhores clientes.

## ğŸ› ï¸ Tecnologias Utilizadas

* **Python 3.14**
* **PySpark** (Processamento distribuÃ­do)
* **Bibliotecas Standard:** `csv`, `os`, `random` (para geraÃ§Ã£o de massa de dados e persistÃªncia local sem dependÃªncia de Hadoop/Winutils).

## ğŸ“‚ Estrutura do Projeto

```text
â”œâ”€â”€ dados/                  # Arquivos de entrada (Gerados via script)
â”œâ”€â”€ output/                 # SaÃ­da do Pipeline
â”‚   â”œâ”€â”€ insights/           # RelatÃ³rios gerenciais (CSV)
â”‚   â”œâ”€â”€ balanco_produtos.csv
â”‚   â””â”€â”€ vendas_detalhadas/  # Data Lake Particionado por Data
â”œâ”€â”€ etl_pipeline.py         # Script Principal
â”œâ”€â”€ gerar_dados.py          # Gerador de Clientes
â”œâ”€â”€ gerar_vendas_massivo.py # Gerador de Vendas (Volume)
â””â”€â”€ README.md